# **[Paper Reading]** Learning to Hash for Indexing Big Data – A Survey

Jun Wang, Wei Liu, Sanjiv Kumar, Shih-Fu Chang

## Introduction
*  Exhaustively comparing a query point with each sample in a database is infeasible because the time complexity, and tree-based approaches require significant storagecosts (sometimes more than the data itself). 
*  Hashing methods are goood.
## Contribution
* Summarizes a lot of hashing methods with their performance in high dimensional space.

## Categories of hashing algorithm
* Data-independent: Locality Sensitie Hashing
* Data-dependent: Learning to Hash

## Learning-based hashing
* **Unsupervised** : attempt to integrate the data properties, such as distributions and manifold structures to design compact hash codes withnimproved accuracy. 
    * Spectral hashing 
    * Graph hashing 
    * Manifold hashing 
    * Iterative quantization hashing 
    * Kernalized locality-sensitive hashing 
    * Isotropic hashing 
    * Angular quantization hashing
    * Spherical hashing
* **Supervised**: from kernel learning to metric learning to deep learning have been exploited to learn binary codes
* **Semi-supervised** learning:　employed to design hash functions　by using both labeled and unlabeled data. 

**Supervised or semi-supervised hashing methods can be further grouped into：**

* Pointwise
* Pairwise
    * ![](https://i.imgur.com/WS66Knh.png)
* Triplet-wise
    * ![](https://i.imgur.com/PY4p9nT.png)
    * ![](https://i.imgur.com/xcXbvMb.png)
* Listwise
    * ![](https://i.imgur.com/IcGm2Ij.png)

**Unlinear usually outperform linear**
* linear hashing often suffers from insufficient discriminative power. Nonlinear methods have been developed to override such limitations.

**Single-Shot Learning vs. Multiple-Shot Learning**
* Single-shot learning:
    * optimal solution is derived by optimizing the objective function in a single-shot, K hash functions are learned simultaneously.
* multiple-shot learning:
    * considers a global objective, but optimizes a hash function considering the bias generated by the previous hash functions.
    * often used in supervised or semi-supervised settings

**Non-Weighted vs.  Weighted Hashing**
* Boosted SimilaritySensitive Coding (BSSC)
    * learning the hash functions and the corresponding weights jointly.
    * objective is to lower the collision probability of non-neighbor pair while improving the collision probability of neighboring pair

## Learning-based hash
![](https://i.imgur.com/nQSeoG5.png)

## Deep learning-based hash
![](https://i.imgur.com/NsQ6Fpp.png)

